{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for Session 9\n",
    "\n",
    "**Note:** This notebook is to be seen as a 'scratch-pad', illustrating the points made in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generation of Some Synthetic Test Data \n",
    "\n",
    "Imagine 400 train and 400 test examples with 10 features each. Each example has **two** labels, label 'a' and label 'b'. We construct these examples synthetically by picking 10 random numbers between 0 and 1, and adding for the examples with label $a =  1$ 0.2 to the first five features, and subtracting 0.2 in those features for examples with label $a = 0$. We do the same for label $b$, where we add/subtract 0.1 to features 6-10. (Obviously, this is just a toy example and the details do not matter - we just want to have some data to explore the formalisms.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1_X = np.array([0.2] * 5 + [0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "train_2_X = np.array([0.2] * 5 + [-0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "train_3_X = np.array([-0.2] * 5 + [0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "train_4_X = np.array([-0.2] * 5 + [-0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "\n",
    "train_X = np.concatenate([train_1_X, train_2_X, train_3_X, train_4_X])\n",
    "\n",
    "test_1_X = np.array([0.2] * 5 + [0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "test_2_X = np.array([0.2] * 5 + [-0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "test_3_X = np.array([-0.2] * 5 + [0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "test_4_X = np.array([-0.2] * 5 + [-0.1] * 5) + np.reshape(np.random.random(1000),[100,10])\n",
    "\n",
    "test_X = np.concatenate([test_1_X, test_2_X, test_3_X, test_4_X])\n",
    "\n",
    "train_1a_y = train_1b_y = train_2a_y = train_3b_y = np.reshape([[1] * 100], [100,1])\n",
    "train_2b_y = train_3a_y = train_4a_y = train_4b_y = np.reshape([[0] * 100], [100,1])\n",
    "\n",
    "test_1a_y = test_1b_y = test_2a_y = test_3b_y = np.reshape([[1] * 100], [100,1])\n",
    "test_2b_y = test_3a_y = test_4a_y = test_4b_y = np.reshape([[0] * 100], [100,1])\n",
    "\n",
    "train_a_y = np.concatenate([train_1a_y, train_2a_y, train_3a_y, train_4a_y])\n",
    "train_b_y = np.concatenate([train_1b_y, train_2b_y, train_3b_y, train_4b_y])\n",
    "\n",
    "test_a_y = np.concatenate([test_1a_y, test_2a_y, test_3a_y, test_4a_y])\n",
    "test_b_y = np.concatenate([test_1b_y, test_2b_y, test_3b_y, test_4b_y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73692999, 1.10728784, 1.00251368, 0.48547276, 1.14743858,\n",
       "        1.0299117 , 0.95879371, 0.40761422, 0.52878452, 0.76707157],\n",
       "       [0.61915175, 1.00161909, 0.80442121, 1.16581909, 1.17072022,\n",
       "        0.56239227, 0.66419048, 0.65646977, 0.99728475, 0.95194047]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_a_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_a_y[:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good.\n",
    "\n",
    "### 2. Keras' Functional API Formalism\n",
    "\n",
    "Keras' Functional API allows us to construct non-sequential models with multiple inputs, multiple outputs, branched models,  etc..\n",
    "\n",
    "Key aspects:\n",
    "\n",
    "* start with definition of inputs through Input layer(s)\n",
    "* define the layers and outputs as:\n",
    "$$\\rm next\\_layer\\_activations = layer(layer\\_parameters) (previous\\_layer\\_activations)$$\n",
    "* define the model the the input(s) and output(s).  \n",
    "* 'compile' the model by specifying loss function(s), metric(s) and the optimizer.  \n",
    "\n",
    "We first set things up for one binary classification. It is most convenient to define a function called *build_model()* where we can parametrize aspects we want to vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1019 10:25:32.000267 4420785600 deprecation.py:506] From /anaconda3/envs/tf1_14/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1019 10:25:32.031944 4420785600 deprecation.py:323] From /anaconda3/envs/tf1_14/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_numbers (InputLayer)   [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "classification (Dense)       (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 231\n",
      "Trainable params: 231\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(activation_function='relu', \n",
    "                optimizer='adam', \n",
    "                kernel_initializer=tf.keras.initializers.he_normal(seed=1), \n",
    "                bias_initializer='zeros'):\n",
    "\n",
    "    # Define Input layer(s)\n",
    "    input_numbers = tf.keras.layers.Input(shape=(10,), name=\"input_numbers\")\n",
    "\n",
    "    # Define one hidden layer and act on input\n",
    "    dense_layer = tf.keras.layers.Dense(10, \n",
    "                                        activation=activation_function, \n",
    "                                        kernel_initializer=kernel_initializer, \n",
    "                                        bias_initializer=bias_initializer,\n",
    "                                        name='dense') # layer definition\n",
    "    dense_output = dense_layer(input_numbers)  # layer acting on previous layer's output\n",
    "    \n",
    "    # Define one hidden layer and act on input\n",
    "    dense_layer_2 = tf.keras.layers.Dense(10, \n",
    "                                        activation=activation_function, \n",
    "                                        kernel_initializer=kernel_initializer, \n",
    "                                        bias_initializer=bias_initializer,\n",
    "                                        name='dense_2') # layer definition\n",
    "    dense_output_2 = dense_layer_2(dense_output)  # layer acting on previous layer's output\n",
    "\n",
    "    \n",
    "\n",
    "    # Define classification layer and act on previous output to classify examples\n",
    "    classification_layer = tf.keras.layers.Dense(1, activation='sigmoid', name='classification') # layer definition\n",
    "    classification_output = classification_layer(dense_output_2)   # layer acting on previous layer's output\n",
    "\n",
    "    # Build and compile model\n",
    "    model = tf.keras.models.Model(input_numbers, classification_output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy!   \n",
    "\n",
    "**Question:** Do the dimensions look right? \n",
    "\n",
    "### 3.  Training Optimizations\n",
    "\n",
    "Great, let's do some experimenting. We run the model above for the synthetic data set we defined, only considering the first label, and look at a few variations. Specifically, let's play with:\n",
    "\n",
    "* the **activation function** in the hidden layer     \n",
    "* the **initialization**    \n",
    "* the **optimizer**\n",
    "\n",
    "We run everything for 40 epochs and compare the results. (We split the training into two parts to cut down on reporting. 'verbose=0' supresses output. But we still want to see the final loss so we add one epoch with 'verbose=1'.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 55us/sample - loss: 0.1802 - val_loss: 0.2568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2c131f98>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = build_model()  # This one should be good...\n",
    "\n",
    "model_1.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=39,\n",
    "        verbose=0)\n",
    "\n",
    "model_1.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our supposed best setup, and it defines our baselines. \n",
    "\n",
    "Let's veer off and see how loss, etc. change. First, we use $tanh()$ as opposed to $relu()$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 96us/sample - loss: 0.2039 - val_loss: 0.2737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2c54e7f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = build_model(activation_function='tanh')  \n",
    "\n",
    "model_2.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=39,\n",
    "        verbose=0)\n",
    "\n",
    "model_2.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit worse. [Of course, the initialization is random, so things may be different on future runs.]\n",
    "\n",
    "Next, we change the optimizer to stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 51us/sample - loss: 0.4382 - val_loss: 0.4778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2c8d96a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = build_model(optimizer='sgd')  \n",
    "\n",
    "model_3.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=39,\n",
    "        verbose=0)\n",
    "\n",
    "model_3.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot worse!!\n",
    "\n",
    "Going back to the activation function, what about $sigmoid()$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.4543 - val_loss: 0.4715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2cb0ebe0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4 = build_model(activation_function='sigmoid')  \n",
    "\n",
    "model_4.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=39,\n",
    "        verbose=0)\n",
    "\n",
    "model_4.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good!\n",
    "\n",
    "Lastly, what about choosing general random matrix initialization as opposed to He/Xavier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1019 10:26:24.996901 4420785600 deprecation.py:506] From /anaconda3/envs/tf1_14/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 99us/sample - loss: 0.1853 - val_loss: 0.2438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2ced67f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5 = build_model(kernel_initializer='random_normal')  \n",
    "\n",
    "model_5.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=39,\n",
    "        verbose=0)\n",
    "\n",
    "model_5.fit(\n",
    "        train_X,\n",
    "        train_a_y,\n",
    "        validation_data=[test_X, test_a_y],\n",
    "        epochs=1,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit worse too.\n",
    "\n",
    "**Critical Note:** We were cheating a bit here! There is a stochastic component and if one re-reruns the cells results will differ. In fact, *in this simple case* the last configuration is often the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Splitting Models - Multiple Outputs\n",
    "\n",
    "Now, let's go back to our dataset and let's try to predict two labels. We want to model that by *splitting* the model, i.e., two distinct hidden layers act on the input, and we use those two branches to predict the two labels:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input layer(s)\n",
    "input_numbers = tf.keras.layers.Input(shape=(10,), name=\"input_numbers\")\n",
    "\n",
    "# Define separate hidden layers and acting on (same!) input\n",
    "dense_layer_1 = tf.keras.layers.Dense(10, activation='relu', name='dense_1')\n",
    "dense_layer_2 = tf.keras.layers.Dense(10, activation='relu', name='dense_2')\n",
    "\n",
    "dense_output_1 = dense_layer_1(input_numbers)     # hideen layer 1 acting on input\n",
    "dense_output_2 = dense_layer_2(input_numbers)     # hideen layer 2 acting on input\n",
    "\n",
    "\n",
    "\n",
    "# Define classification layer and act on previous output to classify examples\n",
    "classification_layer_1 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_1')  # classification layer 1 for first branch\n",
    "\n",
    "classification_layer_2 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_2')  # classification layer 2 for second branch\n",
    "\n",
    "\n",
    "classification_output_1 = classification_layer_1(dense_output_1)\n",
    "classification_output_2 = classification_layer_2(dense_output_2)\n",
    "\n",
    "\n",
    "\n",
    "# Build and compile model\n",
    "\n",
    "model_input = input_numbers\n",
    "model_output = [classification_output_1, classification_output_2]\n",
    "losses = ['binary_crossentropy', 'binary_crossentropy']\n",
    "metrics = ['acc', 'acc']\n",
    "\n",
    "model = tf.keras.models.Model(model_input, model_output)\n",
    "model.compile(loss=losses,  optimizer='adam', metrics=metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_numbers (InputLayer)      [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           110         input_numbers[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           110         input_numbers[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "classification_1 (Dense)        (None, 1)            11          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "classification_2 (Dense)        (None, 1)            11          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 242\n",
      "Trainable params: 242\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "Epoch 1/150\n",
      "400/400 [==============================] - 0s 1ms/sample - loss: 1.4410 - classification_1_loss: 0.7315 - classification_2_loss: 0.7043 - classification_1_acc: 0.5200 - classification_2_acc: 0.4900 - val_loss: 1.4202 - val_classification_1_loss: 0.7183 - val_classification_2_loss: 0.6987 - val_classification_1_acc: 0.5225 - val_classification_2_acc: 0.5150\n",
      "Epoch 2/150\n",
      "400/400 [==============================] - 0s 106us/sample - loss: 1.3913 - classification_1_loss: 0.6991 - classification_2_loss: 0.6954 - classification_1_acc: 0.5400 - classification_2_acc: 0.5300 - val_loss: 1.3781 - val_classification_1_loss: 0.6868 - val_classification_2_loss: 0.6929 - val_classification_1_acc: 0.5700 - val_classification_2_acc: 0.5375\n",
      "Epoch 3/150\n",
      "400/400 [==============================] - 0s 123us/sample - loss: 1.3560 - classification_1_loss: 0.6664 - classification_2_loss: 0.6893 - classification_1_acc: 0.5925 - classification_2_acc: 0.5525 - val_loss: 1.3453 - val_classification_1_loss: 0.6581 - val_classification_2_loss: 0.6849 - val_classification_1_acc: 0.6500 - val_classification_2_acc: 0.5425\n",
      "Epoch 4/150\n",
      "400/400 [==============================] - 0s 116us/sample - loss: 1.3289 - classification_1_loss: 0.6444 - classification_2_loss: 0.6839 - classification_1_acc: 0.6500 - classification_2_acc: 0.5800 - val_loss: 1.3216 - val_classification_1_loss: 0.6422 - val_classification_2_loss: 0.6827 - val_classification_1_acc: 0.6875 - val_classification_2_acc: 0.5550\n",
      "Epoch 5/150\n",
      "400/400 [==============================] - 0s 118us/sample - loss: 1.3071 - classification_1_loss: 0.6308 - classification_2_loss: 0.6786 - classification_1_acc: 0.6750 - classification_2_acc: 0.5925 - val_loss: 1.3029 - val_classification_1_loss: 0.6270 - val_classification_2_loss: 0.6774 - val_classification_1_acc: 0.7125 - val_classification_2_acc: 0.5550\n",
      "Epoch 6/150\n",
      "400/400 [==============================] - 0s 167us/sample - loss: 1.2893 - classification_1_loss: 0.6168 - classification_2_loss: 0.6750 - classification_1_acc: 0.7225 - classification_2_acc: 0.6025 - val_loss: 1.2866 - val_classification_1_loss: 0.6131 - val_classification_2_loss: 0.6724 - val_classification_1_acc: 0.7450 - val_classification_2_acc: 0.5725\n",
      "Epoch 7/150\n",
      "400/400 [==============================] - 0s 164us/sample - loss: 1.2732 - classification_1_loss: 0.6029 - classification_2_loss: 0.6697 - classification_1_acc: 0.7375 - classification_2_acc: 0.6125 - val_loss: 1.2713 - val_classification_1_loss: 0.6012 - val_classification_2_loss: 0.6682 - val_classification_1_acc: 0.7625 - val_classification_2_acc: 0.5850\n",
      "Epoch 8/150\n",
      "400/400 [==============================] - 0s 125us/sample - loss: 1.2574 - classification_1_loss: 0.5880 - classification_2_loss: 0.6676 - classification_1_acc: 0.7600 - classification_2_acc: 0.6200 - val_loss: 1.2559 - val_classification_1_loss: 0.5913 - val_classification_2_loss: 0.6665 - val_classification_1_acc: 0.7875 - val_classification_2_acc: 0.6025\n",
      "Epoch 9/150\n",
      "400/400 [==============================] - 0s 113us/sample - loss: 1.2415 - classification_1_loss: 0.5782 - classification_2_loss: 0.6667 - classification_1_acc: 0.7725 - classification_2_acc: 0.6250 - val_loss: 1.2404 - val_classification_1_loss: 0.5751 - val_classification_2_loss: 0.6612 - val_classification_1_acc: 0.8000 - val_classification_2_acc: 0.6075\n",
      "Epoch 10/150\n",
      "400/400 [==============================] - 0s 116us/sample - loss: 1.2253 - classification_1_loss: 0.5620 - classification_2_loss: 0.6593 - classification_1_acc: 0.7875 - classification_2_acc: 0.6375 - val_loss: 1.2255 - val_classification_1_loss: 0.5640 - val_classification_2_loss: 0.6580 - val_classification_1_acc: 0.8125 - val_classification_2_acc: 0.6100\n",
      "Epoch 11/150\n",
      "400/400 [==============================] - 0s 110us/sample - loss: 1.2101 - classification_1_loss: 0.5560 - classification_2_loss: 0.6574 - classification_1_acc: 0.7925 - classification_2_acc: 0.6450 - val_loss: 1.2111 - val_classification_1_loss: 0.5524 - val_classification_2_loss: 0.6563 - val_classification_1_acc: 0.8225 - val_classification_2_acc: 0.6225\n",
      "Epoch 12/150\n",
      "400/400 [==============================] - 0s 112us/sample - loss: 1.1943 - classification_1_loss: 0.5402 - classification_2_loss: 0.6515 - classification_1_acc: 0.8000 - classification_2_acc: 0.6450 - val_loss: 1.1965 - val_classification_1_loss: 0.5449 - val_classification_2_loss: 0.6536 - val_classification_1_acc: 0.8325 - val_classification_2_acc: 0.6375\n",
      "Epoch 13/150\n",
      "400/400 [==============================] - 0s 110us/sample - loss: 1.1790 - classification_1_loss: 0.5294 - classification_2_loss: 0.6469 - classification_1_acc: 0.8175 - classification_2_acc: 0.6625 - val_loss: 1.1823 - val_classification_1_loss: 0.5309 - val_classification_2_loss: 0.6486 - val_classification_1_acc: 0.8275 - val_classification_2_acc: 0.6300\n",
      "Epoch 14/150\n",
      "400/400 [==============================] - 0s 108us/sample - loss: 1.1641 - classification_1_loss: 0.5174 - classification_2_loss: 0.6462 - classification_1_acc: 0.8200 - classification_2_acc: 0.6650 - val_loss: 1.1682 - val_classification_1_loss: 0.5187 - val_classification_2_loss: 0.6447 - val_classification_1_acc: 0.8400 - val_classification_2_acc: 0.6375\n",
      "Epoch 15/150\n",
      "400/400 [==============================] - 0s 105us/sample - loss: 1.1496 - classification_1_loss: 0.5056 - classification_2_loss: 0.6420 - classification_1_acc: 0.8250 - classification_2_acc: 0.6675 - val_loss: 1.1543 - val_classification_1_loss: 0.5105 - val_classification_2_loss: 0.6429 - val_classification_1_acc: 0.8425 - val_classification_2_acc: 0.6425\n",
      "Epoch 16/150\n",
      "400/400 [==============================] - 0s 114us/sample - loss: 1.1347 - classification_1_loss: 0.4960 - classification_2_loss: 0.6367 - classification_1_acc: 0.8350 - classification_2_acc: 0.6800 - val_loss: 1.1409 - val_classification_1_loss: 0.4988 - val_classification_2_loss: 0.6394 - val_classification_1_acc: 0.8450 - val_classification_2_acc: 0.6375\n",
      "Epoch 17/150\n",
      "400/400 [==============================] - 0s 112us/sample - loss: 1.1210 - classification_1_loss: 0.4819 - classification_2_loss: 0.6346 - classification_1_acc: 0.8450 - classification_2_acc: 0.6650 - val_loss: 1.1277 - val_classification_1_loss: 0.4928 - val_classification_2_loss: 0.6381 - val_classification_1_acc: 0.8475 - val_classification_2_acc: 0.6525\n",
      "Epoch 18/150\n",
      "400/400 [==============================] - 0s 126us/sample - loss: 1.1069 - classification_1_loss: 0.4732 - classification_2_loss: 0.6330 - classification_1_acc: 0.8450 - classification_2_acc: 0.6675 - val_loss: 1.1147 - val_classification_1_loss: 0.4796 - val_classification_2_loss: 0.6329 - val_classification_1_acc: 0.8550 - val_classification_2_acc: 0.6525\n",
      "Epoch 19/150\n",
      "400/400 [==============================] - 0s 278us/sample - loss: 1.0934 - classification_1_loss: 0.4615 - classification_2_loss: 0.6293 - classification_1_acc: 0.8450 - classification_2_acc: 0.6750 - val_loss: 1.1022 - val_classification_1_loss: 0.4725 - val_classification_2_loss: 0.6339 - val_classification_1_acc: 0.8525 - val_classification_2_acc: 0.6550\n",
      "Epoch 20/150\n",
      "400/400 [==============================] - 0s 322us/sample - loss: 1.0806 - classification_1_loss: 0.4539 - classification_2_loss: 0.6260 - classification_1_acc: 0.8500 - classification_2_acc: 0.6725 - val_loss: 1.0899 - val_classification_1_loss: 0.4614 - val_classification_2_loss: 0.6334 - val_classification_1_acc: 0.8550 - val_classification_2_acc: 0.6625\n",
      "Epoch 21/150\n",
      "400/400 [==============================] - 0s 265us/sample - loss: 1.0672 - classification_1_loss: 0.4457 - classification_2_loss: 0.6227 - classification_1_acc: 0.8525 - classification_2_acc: 0.6750 - val_loss: 1.0780 - val_classification_1_loss: 0.4464 - val_classification_2_loss: 0.6233 - val_classification_1_acc: 0.8550 - val_classification_2_acc: 0.6650\n",
      "Epoch 22/150\n",
      "400/400 [==============================] - 0s 265us/sample - loss: 1.0548 - classification_1_loss: 0.4354 - classification_2_loss: 0.6218 - classification_1_acc: 0.8650 - classification_2_acc: 0.6825 - val_loss: 1.0668 - val_classification_1_loss: 0.4445 - val_classification_2_loss: 0.6275 - val_classification_1_acc: 0.8525 - val_classification_2_acc: 0.6600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/150\n",
      "400/400 [==============================] - ETA: 0s - loss: 1.0209 - classification_1_loss: 0.4413 - classification_2_loss: 0.5795 - classification_1_acc: 0.8438 - classification_2_acc: 0.750 - 0s 190us/sample - loss: 1.0429 - classification_1_loss: 0.4265 - classification_2_loss: 0.6215 - classification_1_acc: 0.8650 - classification_2_acc: 0.6900 - val_loss: 1.0555 - val_classification_1_loss: 0.4372 - val_classification_2_loss: 0.6223 - val_classification_1_acc: 0.8600 - val_classification_2_acc: 0.6650\n",
      "Epoch 24/150\n",
      "400/400 [==============================] - 0s 132us/sample - loss: 1.0308 - classification_1_loss: 0.4200 - classification_2_loss: 0.6182 - classification_1_acc: 0.8700 - classification_2_acc: 0.6950 - val_loss: 1.0446 - val_classification_1_loss: 0.4231 - val_classification_2_loss: 0.6215 - val_classification_1_acc: 0.8600 - val_classification_2_acc: 0.6575\n",
      "Epoch 25/150\n",
      "400/400 [==============================] - 0s 135us/sample - loss: 1.0192 - classification_1_loss: 0.4041 - classification_2_loss: 0.6152 - classification_1_acc: 0.8725 - classification_2_acc: 0.6825 - val_loss: 1.0347 - val_classification_1_loss: 0.4176 - val_classification_2_loss: 0.6205 - val_classification_1_acc: 0.8575 - val_classification_2_acc: 0.6625\n",
      "Epoch 26/150\n",
      "400/400 [==============================] - 0s 122us/sample - loss: 1.0086 - classification_1_loss: 0.3970 - classification_2_loss: 0.6087 - classification_1_acc: 0.8750 - classification_2_acc: 0.6900 - val_loss: 1.0248 - val_classification_1_loss: 0.4092 - val_classification_2_loss: 0.6142 - val_classification_1_acc: 0.8575 - val_classification_2_acc: 0.6600\n",
      "Epoch 27/150\n",
      "400/400 [==============================] - 0s 153us/sample - loss: 0.9985 - classification_1_loss: 0.3902 - classification_2_loss: 0.6052 - classification_1_acc: 0.8700 - classification_2_acc: 0.6950 - val_loss: 1.0153 - val_classification_1_loss: 0.4006 - val_classification_2_loss: 0.6102 - val_classification_1_acc: 0.8575 - val_classification_2_acc: 0.6625\n",
      "Epoch 28/150\n",
      "400/400 [==============================] - 0s 122us/sample - loss: 0.9879 - classification_1_loss: 0.3822 - classification_2_loss: 0.6041 - classification_1_acc: 0.8750 - classification_2_acc: 0.7025 - val_loss: 1.0062 - val_classification_1_loss: 0.3962 - val_classification_2_loss: 0.6111 - val_classification_1_acc: 0.8600 - val_classification_2_acc: 0.6625\n",
      "Epoch 29/150\n",
      "400/400 [==============================] - 0s 127us/sample - loss: 0.9779 - classification_1_loss: 0.3733 - classification_2_loss: 0.6023 - classification_1_acc: 0.8725 - classification_2_acc: 0.6975 - val_loss: 0.9972 - val_classification_1_loss: 0.3875 - val_classification_2_loss: 0.6074 - val_classification_1_acc: 0.8600 - val_classification_2_acc: 0.6675\n",
      "Epoch 30/150\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 0.9687 - classification_1_loss: 0.3747 - classification_2_loss: 0.6039 - classification_1_acc: 0.8775 - classification_2_acc: 0.6975 - val_loss: 0.9888 - val_classification_1_loss: 0.3799 - val_classification_2_loss: 0.6060 - val_classification_1_acc: 0.8600 - val_classification_2_acc: 0.6700\n",
      "Epoch 31/150\n",
      "400/400 [==============================] - 0s 126us/sample - loss: 0.9597 - classification_1_loss: 0.3627 - classification_2_loss: 0.6031 - classification_1_acc: 0.8775 - classification_2_acc: 0.7000 - val_loss: 0.9810 - val_classification_1_loss: 0.3786 - val_classification_2_loss: 0.6021 - val_classification_1_acc: 0.8650 - val_classification_2_acc: 0.6700\n",
      "Epoch 32/150\n",
      "400/400 [==============================] - 0s 82us/sample - loss: 0.9513 - classification_1_loss: 0.3581 - classification_2_loss: 0.5909 - classification_1_acc: 0.8800 - classification_2_acc: 0.7025 - val_loss: 0.9732 - val_classification_1_loss: 0.3745 - val_classification_2_loss: 0.6003 - val_classification_1_acc: 0.8625 - val_classification_2_acc: 0.6750\n",
      "Epoch 33/150\n",
      "400/400 [==============================] - 0s 108us/sample - loss: 0.9430 - classification_1_loss: 0.3494 - classification_2_loss: 0.5947 - classification_1_acc: 0.8800 - classification_2_acc: 0.7075 - val_loss: 0.9659 - val_classification_1_loss: 0.3721 - val_classification_2_loss: 0.5997 - val_classification_1_acc: 0.8625 - val_classification_2_acc: 0.6725\n",
      "Epoch 34/150\n",
      "400/400 [==============================] - 0s 120us/sample - loss: 0.9347 - classification_1_loss: 0.3455 - classification_2_loss: 0.5908 - classification_1_acc: 0.8825 - classification_2_acc: 0.7025 - val_loss: 0.9588 - val_classification_1_loss: 0.3571 - val_classification_2_loss: 0.6011 - val_classification_1_acc: 0.8625 - val_classification_2_acc: 0.6725\n",
      "Epoch 35/150\n",
      "400/400 [==============================] - 0s 126us/sample - loss: 0.9270 - classification_1_loss: 0.3390 - classification_2_loss: 0.5900 - classification_1_acc: 0.8825 - classification_2_acc: 0.7100 - val_loss: 0.9519 - val_classification_1_loss: 0.3601 - val_classification_2_loss: 0.5983 - val_classification_1_acc: 0.8650 - val_classification_2_acc: 0.6725\n",
      "Epoch 36/150\n",
      "400/400 [==============================] - 0s 147us/sample - loss: 0.9193 - classification_1_loss: 0.3321 - classification_2_loss: 0.5832 - classification_1_acc: 0.8800 - classification_2_acc: 0.7150 - val_loss: 0.9454 - val_classification_1_loss: 0.3502 - val_classification_2_loss: 0.5957 - val_classification_1_acc: 0.8675 - val_classification_2_acc: 0.6750\n",
      "Epoch 37/150\n",
      "400/400 [==============================] - 0s 138us/sample - loss: 0.9129 - classification_1_loss: 0.3228 - classification_2_loss: 0.5854 - classification_1_acc: 0.8775 - classification_2_acc: 0.7175 - val_loss: 0.9391 - val_classification_1_loss: 0.3508 - val_classification_2_loss: 0.5881 - val_classification_1_acc: 0.8675 - val_classification_2_acc: 0.6800\n",
      "Epoch 38/150\n",
      "400/400 [==============================] - 0s 148us/sample - loss: 0.9047 - classification_1_loss: 0.3253 - classification_2_loss: 0.5799 - classification_1_acc: 0.8825 - classification_2_acc: 0.7100 - val_loss: 0.9332 - val_classification_1_loss: 0.3420 - val_classification_2_loss: 0.5906 - val_classification_1_acc: 0.8700 - val_classification_2_acc: 0.6750\n",
      "Epoch 39/150\n",
      "400/400 [==============================] - 0s 160us/sample - loss: 0.8985 - classification_1_loss: 0.3163 - classification_2_loss: 0.5809 - classification_1_acc: 0.8800 - classification_2_acc: 0.7075 - val_loss: 0.9272 - val_classification_1_loss: 0.3440 - val_classification_2_loss: 0.5867 - val_classification_1_acc: 0.8700 - val_classification_2_acc: 0.6775\n",
      "Epoch 40/150\n",
      "400/400 [==============================] - 0s 124us/sample - loss: 0.8915 - classification_1_loss: 0.3118 - classification_2_loss: 0.5746 - classification_1_acc: 0.8850 - classification_2_acc: 0.7075 - val_loss: 0.9212 - val_classification_1_loss: 0.3364 - val_classification_2_loss: 0.5896 - val_classification_1_acc: 0.8675 - val_classification_2_acc: 0.6875\n",
      "Epoch 41/150\n",
      "400/400 [==============================] - 0s 129us/sample - loss: 0.8846 - classification_1_loss: 0.3107 - classification_2_loss: 0.5756 - classification_1_acc: 0.8825 - classification_2_acc: 0.7075 - val_loss: 0.9154 - val_classification_1_loss: 0.3325 - val_classification_2_loss: 0.5853 - val_classification_1_acc: 0.8700 - val_classification_2_acc: 0.6900\n",
      "Epoch 42/150\n",
      "400/400 [==============================] - 0s 122us/sample - loss: 0.8781 - classification_1_loss: 0.2996 - classification_2_loss: 0.5717 - classification_1_acc: 0.8875 - classification_2_acc: 0.7050 - val_loss: 0.9103 - val_classification_1_loss: 0.3319 - val_classification_2_loss: 0.5916 - val_classification_1_acc: 0.8725 - val_classification_2_acc: 0.6950\n",
      "Epoch 43/150\n",
      "400/400 [==============================] - 0s 135us/sample - loss: 0.8721 - classification_1_loss: 0.3012 - classification_2_loss: 0.5758 - classification_1_acc: 0.8875 - classification_2_acc: 0.7050 - val_loss: 0.9052 - val_classification_1_loss: 0.3204 - val_classification_2_loss: 0.5826 - val_classification_1_acc: 0.8725 - val_classification_2_acc: 0.6925\n",
      "Epoch 44/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 0s 111us/sample - loss: 0.8666 - classification_1_loss: 0.2977 - classification_2_loss: 0.5727 - classification_1_acc: 0.8875 - classification_2_acc: 0.7125 - val_loss: 0.8994 - val_classification_1_loss: 0.3192 - val_classification_2_loss: 0.5792 - val_classification_1_acc: 0.8725 - val_classification_2_acc: 0.6950\n",
      "Epoch 45/150\n",
      "400/400 [==============================] - 0s 108us/sample - loss: 0.8602 - classification_1_loss: 0.2964 - classification_2_loss: 0.5647 - classification_1_acc: 0.9000 - classification_2_acc: 0.7225 - val_loss: 0.8949 - val_classification_1_loss: 0.3168 - val_classification_2_loss: 0.5815 - val_classification_1_acc: 0.8800 - val_classification_2_acc: 0.6925\n",
      "Epoch 46/150\n",
      "400/400 [==============================] - 0s 123us/sample - loss: 0.8548 - classification_1_loss: 0.2892 - classification_2_loss: 0.5697 - classification_1_acc: 0.9025 - classification_2_acc: 0.7075 - val_loss: 0.8903 - val_classification_1_loss: 0.3114 - val_classification_2_loss: 0.5767 - val_classification_1_acc: 0.8775 - val_classification_2_acc: 0.6975\n",
      "Epoch 47/150\n",
      "400/400 [==============================] - 0s 148us/sample - loss: 0.8486 - classification_1_loss: 0.2862 - classification_2_loss: 0.5682 - classification_1_acc: 0.9025 - classification_2_acc: 0.7200 - val_loss: 0.8852 - val_classification_1_loss: 0.3112 - val_classification_2_loss: 0.5736 - val_classification_1_acc: 0.8800 - val_classification_2_acc: 0.6975\n",
      "Epoch 48/150\n",
      "400/400 [==============================] - 0s 165us/sample - loss: 0.8430 - classification_1_loss: 0.2799 - classification_2_loss: 0.5666 - classification_1_acc: 0.9025 - classification_2_acc: 0.7250 - val_loss: 0.8806 - val_classification_1_loss: 0.3022 - val_classification_2_loss: 0.5683 - val_classification_1_acc: 0.8775 - val_classification_2_acc: 0.6975\n",
      "Epoch 49/150\n",
      "400/400 [==============================] - 0s 186us/sample - loss: 0.8379 - classification_1_loss: 0.2796 - classification_2_loss: 0.5614 - classification_1_acc: 0.9050 - classification_2_acc: 0.7250 - val_loss: 0.8764 - val_classification_1_loss: 0.3006 - val_classification_2_loss: 0.5682 - val_classification_1_acc: 0.8800 - val_classification_2_acc: 0.6975\n",
      "Epoch 50/150\n",
      "400/400 [==============================] - 0s 145us/sample - loss: 0.8329 - classification_1_loss: 0.2760 - classification_2_loss: 0.5542 - classification_1_acc: 0.9050 - classification_2_acc: 0.7275 - val_loss: 0.8720 - val_classification_1_loss: 0.3018 - val_classification_2_loss: 0.5684 - val_classification_1_acc: 0.8800 - val_classification_2_acc: 0.7025\n",
      "Epoch 51/150\n",
      "400/400 [==============================] - 0s 147us/sample - loss: 0.8282 - classification_1_loss: 0.2674 - classification_2_loss: 0.5563 - classification_1_acc: 0.9025 - classification_2_acc: 0.7325 - val_loss: 0.8679 - val_classification_1_loss: 0.2968 - val_classification_2_loss: 0.5677 - val_classification_1_acc: 0.8800 - val_classification_2_acc: 0.7075\n",
      "Epoch 52/150\n",
      "400/400 [==============================] - 0s 156us/sample - loss: 0.8233 - classification_1_loss: 0.2681 - classification_2_loss: 0.5516 - classification_1_acc: 0.9075 - classification_2_acc: 0.7325 - val_loss: 0.8641 - val_classification_1_loss: 0.2957 - val_classification_2_loss: 0.5674 - val_classification_1_acc: 0.8825 - val_classification_2_acc: 0.7050\n",
      "Epoch 53/150\n",
      "400/400 [==============================] - 0s 138us/sample - loss: 0.8186 - classification_1_loss: 0.2659 - classification_2_loss: 0.5620 - classification_1_acc: 0.9075 - classification_2_acc: 0.7250 - val_loss: 0.8605 - val_classification_1_loss: 0.2963 - val_classification_2_loss: 0.5639 - val_classification_1_acc: 0.8800 - val_classification_2_acc: 0.7075\n",
      "Epoch 54/150\n",
      "400/400 [==============================] - 0s 172us/sample - loss: 0.8137 - classification_1_loss: 0.2707 - classification_2_loss: 0.5553 - classification_1_acc: 0.9050 - classification_2_acc: 0.7325 - val_loss: 0.8573 - val_classification_1_loss: 0.2950 - val_classification_2_loss: 0.5628 - val_classification_1_acc: 0.8825 - val_classification_2_acc: 0.7100\n",
      "Epoch 55/150\n",
      "400/400 [==============================] - 0s 141us/sample - loss: 0.8083 - classification_1_loss: 0.2567 - classification_2_loss: 0.5487 - classification_1_acc: 0.9100 - classification_2_acc: 0.7325 - val_loss: 0.8532 - val_classification_1_loss: 0.2886 - val_classification_2_loss: 0.5621 - val_classification_1_acc: 0.8800 - val_classification_2_acc: 0.7125\n",
      "Epoch 56/150\n",
      "400/400 [==============================] - 0s 94us/sample - loss: 0.8050 - classification_1_loss: 0.2540 - classification_2_loss: 0.5509 - classification_1_acc: 0.9100 - classification_2_acc: 0.7350 - val_loss: 0.8497 - val_classification_1_loss: 0.2906 - val_classification_2_loss: 0.5584 - val_classification_1_acc: 0.8800 - val_classification_2_acc: 0.7225\n",
      "Epoch 57/150\n",
      "400/400 [==============================] - 0s 87us/sample - loss: 0.8009 - classification_1_loss: 0.2570 - classification_2_loss: 0.5547 - classification_1_acc: 0.9075 - classification_2_acc: 0.7325 - val_loss: 0.8457 - val_classification_1_loss: 0.2865 - val_classification_2_loss: 0.5594 - val_classification_1_acc: 0.8850 - val_classification_2_acc: 0.7200\n",
      "Epoch 58/150\n",
      "400/400 [==============================] - 0s 119us/sample - loss: 0.7979 - classification_1_loss: 0.2536 - classification_2_loss: 0.5461 - classification_1_acc: 0.9100 - classification_2_acc: 0.7375 - val_loss: 0.8440 - val_classification_1_loss: 0.2827 - val_classification_2_loss: 0.5614 - val_classification_1_acc: 0.8825 - val_classification_2_acc: 0.7225\n",
      "Epoch 59/150\n",
      "400/400 [==============================] - 0s 122us/sample - loss: 0.7926 - classification_1_loss: 0.2468 - classification_2_loss: 0.5475 - classification_1_acc: 0.9050 - classification_2_acc: 0.7375 - val_loss: 0.8398 - val_classification_1_loss: 0.2930 - val_classification_2_loss: 0.5526 - val_classification_1_acc: 0.8825 - val_classification_2_acc: 0.7200\n",
      "Epoch 60/150\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.7883 - classification_1_loss: 0.2442 - classification_2_loss: 0.5421 - classification_1_acc: 0.9125 - classification_2_acc: 0.7425 - val_loss: 0.8369 - val_classification_1_loss: 0.2891 - val_classification_2_loss: 0.5537 - val_classification_1_acc: 0.8875 - val_classification_2_acc: 0.7225\n",
      "Epoch 61/150\n",
      "400/400 [==============================] - 0s 69us/sample - loss: 0.7847 - classification_1_loss: 0.2492 - classification_2_loss: 0.5379 - classification_1_acc: 0.9125 - classification_2_acc: 0.7400 - val_loss: 0.8334 - val_classification_1_loss: 0.2812 - val_classification_2_loss: 0.5535 - val_classification_1_acc: 0.8875 - val_classification_2_acc: 0.7225\n",
      "Epoch 62/150\n",
      "400/400 [==============================] - 0s 127us/sample - loss: 0.7818 - classification_1_loss: 0.2440 - classification_2_loss: 0.5391 - classification_1_acc: 0.9100 - classification_2_acc: 0.7350 - val_loss: 0.8314 - val_classification_1_loss: 0.2776 - val_classification_2_loss: 0.5502 - val_classification_1_acc: 0.8900 - val_classification_2_acc: 0.7250\n",
      "Epoch 63/150\n",
      "400/400 [==============================] - 0s 79us/sample - loss: 0.7769 - classification_1_loss: 0.2433 - classification_2_loss: 0.5376 - classification_1_acc: 0.9100 - classification_2_acc: 0.7425 - val_loss: 0.8279 - val_classification_1_loss: 0.2746 - val_classification_2_loss: 0.5509 - val_classification_1_acc: 0.8825 - val_classification_2_acc: 0.7350\n",
      "Epoch 64/150\n",
      "400/400 [==============================] - 0s 104us/sample - loss: 0.7738 - classification_1_loss: 0.2352 - classification_2_loss: 0.5357 - classification_1_acc: 0.9125 - classification_2_acc: 0.7475 - val_loss: 0.8245 - val_classification_1_loss: 0.2761 - val_classification_2_loss: 0.5471 - val_classification_1_acc: 0.8900 - val_classification_2_acc: 0.7350\n",
      "Epoch 65/150\n",
      "400/400 [==============================] - 0s 107us/sample - loss: 0.7705 - classification_1_loss: 0.2308 - classification_2_loss: 0.5404 - classification_1_acc: 0.9150 - classification_2_acc: 0.7475 - val_loss: 0.8227 - val_classification_1_loss: 0.2853 - val_classification_2_loss: 0.5447 - val_classification_1_acc: 0.8850 - val_classification_2_acc: 0.7350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/150\n",
      "400/400 [==============================] - 0s 111us/sample - loss: 0.7668 - classification_1_loss: 0.2290 - classification_2_loss: 0.5349 - classification_1_acc: 0.9125 - classification_2_acc: 0.7500 - val_loss: 0.8193 - val_classification_1_loss: 0.2808 - val_classification_2_loss: 0.5470 - val_classification_1_acc: 0.8875 - val_classification_2_acc: 0.7350\n",
      "Epoch 67/150\n",
      "400/400 [==============================] - 0s 188us/sample - loss: 0.7636 - classification_1_loss: 0.2356 - classification_2_loss: 0.5285 - classification_1_acc: 0.9125 - classification_2_acc: 0.7475 - val_loss: 0.8169 - val_classification_1_loss: 0.2786 - val_classification_2_loss: 0.5423 - val_classification_1_acc: 0.8925 - val_classification_2_acc: 0.7350\n",
      "Epoch 68/150\n",
      "400/400 [==============================] - 0s 354us/sample - loss: 0.7601 - classification_1_loss: 0.2281 - classification_2_loss: 0.5293 - classification_1_acc: 0.9125 - classification_2_acc: 0.7500 - val_loss: 0.8141 - val_classification_1_loss: 0.2713 - val_classification_2_loss: 0.5441 - val_classification_1_acc: 0.8900 - val_classification_2_acc: 0.7325\n",
      "Epoch 69/150\n",
      "400/400 [==============================] - 0s 332us/sample - loss: 0.7570 - classification_1_loss: 0.2258 - classification_2_loss: 0.5297 - classification_1_acc: 0.9125 - classification_2_acc: 0.7500 - val_loss: 0.8115 - val_classification_1_loss: 0.2724 - val_classification_2_loss: 0.5394 - val_classification_1_acc: 0.8900 - val_classification_2_acc: 0.7325\n",
      "Epoch 70/150\n",
      "400/400 [==============================] - 0s 154us/sample - loss: 0.7538 - classification_1_loss: 0.2236 - classification_2_loss: 0.5255 - classification_1_acc: 0.9125 - classification_2_acc: 0.7500 - val_loss: 0.8091 - val_classification_1_loss: 0.2697 - val_classification_2_loss: 0.5469 - val_classification_1_acc: 0.8900 - val_classification_2_acc: 0.7350\n",
      "Epoch 71/150\n",
      "400/400 [==============================] - 0s 161us/sample - loss: 0.7509 - classification_1_loss: 0.2267 - classification_2_loss: 0.5218 - classification_1_acc: 0.9150 - classification_2_acc: 0.7500 - val_loss: 0.8075 - val_classification_1_loss: 0.2654 - val_classification_2_loss: 0.5462 - val_classification_1_acc: 0.8875 - val_classification_2_acc: 0.7350\n",
      "Epoch 72/150\n",
      "400/400 [==============================] - 0s 160us/sample - loss: 0.7478 - classification_1_loss: 0.2192 - classification_2_loss: 0.5257 - classification_1_acc: 0.9150 - classification_2_acc: 0.7525 - val_loss: 0.8042 - val_classification_1_loss: 0.2676 - val_classification_2_loss: 0.5388 - val_classification_1_acc: 0.8950 - val_classification_2_acc: 0.7375\n",
      "Epoch 73/150\n",
      "400/400 [==============================] - 0s 136us/sample - loss: 0.7451 - classification_1_loss: 0.2233 - classification_2_loss: 0.5366 - classification_1_acc: 0.9175 - classification_2_acc: 0.7575 - val_loss: 0.8022 - val_classification_1_loss: 0.2670 - val_classification_2_loss: 0.5375 - val_classification_1_acc: 0.8900 - val_classification_2_acc: 0.7375\n",
      "Epoch 74/150\n",
      "400/400 [==============================] - 0s 135us/sample - loss: 0.7421 - classification_1_loss: 0.2247 - classification_2_loss: 0.5254 - classification_1_acc: 0.9150 - classification_2_acc: 0.7575 - val_loss: 0.8006 - val_classification_1_loss: 0.2586 - val_classification_2_loss: 0.5342 - val_classification_1_acc: 0.8925 - val_classification_2_acc: 0.7350\n",
      "Epoch 75/150\n",
      "400/400 [==============================] - 0s 139us/sample - loss: 0.7393 - classification_1_loss: 0.2136 - classification_2_loss: 0.5262 - classification_1_acc: 0.9175 - classification_2_acc: 0.7500 - val_loss: 0.7993 - val_classification_1_loss: 0.2578 - val_classification_2_loss: 0.5326 - val_classification_1_acc: 0.8925 - val_classification_2_acc: 0.7350\n",
      "Epoch 76/150\n",
      "400/400 [==============================] - 0s 145us/sample - loss: 0.7364 - classification_1_loss: 0.2174 - classification_2_loss: 0.5203 - classification_1_acc: 0.9150 - classification_2_acc: 0.7475 - val_loss: 0.7967 - val_classification_1_loss: 0.2647 - val_classification_2_loss: 0.5317 - val_classification_1_acc: 0.8925 - val_classification_2_acc: 0.7400\n",
      "Epoch 77/150\n",
      "400/400 [==============================] - 0s 144us/sample - loss: 0.7338 - classification_1_loss: 0.2134 - classification_2_loss: 0.5203 - classification_1_acc: 0.9150 - classification_2_acc: 0.7550 - val_loss: 0.7933 - val_classification_1_loss: 0.2651 - val_classification_2_loss: 0.5364 - val_classification_1_acc: 0.8925 - val_classification_2_acc: 0.7375\n",
      "Epoch 78/150\n",
      "400/400 [==============================] - 0s 148us/sample - loss: 0.7314 - classification_1_loss: 0.2171 - classification_2_loss: 0.5227 - classification_1_acc: 0.9200 - classification_2_acc: 0.7550 - val_loss: 0.7908 - val_classification_1_loss: 0.2544 - val_classification_2_loss: 0.5323 - val_classification_1_acc: 0.8950 - val_classification_2_acc: 0.7350\n",
      "Epoch 79/150\n",
      "400/400 [==============================] - 0s 153us/sample - loss: 0.7296 - classification_1_loss: 0.2132 - classification_2_loss: 0.5208 - classification_1_acc: 0.9175 - classification_2_acc: 0.7525 - val_loss: 0.7901 - val_classification_1_loss: 0.2600 - val_classification_2_loss: 0.5309 - val_classification_1_acc: 0.8975 - val_classification_2_acc: 0.7375\n",
      "Epoch 80/150\n",
      "400/400 [==============================] - 0s 175us/sample - loss: 0.7257 - classification_1_loss: 0.2088 - classification_2_loss: 0.5201 - classification_1_acc: 0.9200 - classification_2_acc: 0.7575 - val_loss: 0.7871 - val_classification_1_loss: 0.2644 - val_classification_2_loss: 0.5283 - val_classification_1_acc: 0.8950 - val_classification_2_acc: 0.7400\n",
      "Epoch 81/150\n",
      "400/400 [==============================] - 0s 149us/sample - loss: 0.7240 - classification_1_loss: 0.2105 - classification_2_loss: 0.5139 - classification_1_acc: 0.9175 - classification_2_acc: 0.7550 - val_loss: 0.7863 - val_classification_1_loss: 0.2526 - val_classification_2_loss: 0.5320 - val_classification_1_acc: 0.8950 - val_classification_2_acc: 0.7400\n",
      "Epoch 82/150\n",
      "400/400 [==============================] - 0s 211us/sample - loss: 0.7210 - classification_1_loss: 0.2076 - classification_2_loss: 0.5120 - classification_1_acc: 0.9200 - classification_2_acc: 0.7575 - val_loss: 0.7842 - val_classification_1_loss: 0.2558 - val_classification_2_loss: 0.5251 - val_classification_1_acc: 0.8950 - val_classification_2_acc: 0.7425\n",
      "Epoch 83/150\n",
      "400/400 [==============================] - 0s 155us/sample - loss: 0.7187 - classification_1_loss: 0.2062 - classification_2_loss: 0.5218 - classification_1_acc: 0.9200 - classification_2_acc: 0.7575 - val_loss: 0.7832 - val_classification_1_loss: 0.2586 - val_classification_2_loss: 0.5208 - val_classification_1_acc: 0.8950 - val_classification_2_acc: 0.7400\n",
      "Epoch 84/150\n",
      "400/400 [==============================] - 0s 174us/sample - loss: 0.7165 - classification_1_loss: 0.2023 - classification_2_loss: 0.5193 - classification_1_acc: 0.9175 - classification_2_acc: 0.7550 - val_loss: 0.7793 - val_classification_1_loss: 0.2498 - val_classification_2_loss: 0.5207 - val_classification_1_acc: 0.8975 - val_classification_2_acc: 0.7350\n",
      "Epoch 85/150\n",
      "400/400 [==============================] - 0s 198us/sample - loss: 0.7136 - classification_1_loss: 0.2008 - classification_2_loss: 0.5096 - classification_1_acc: 0.9200 - classification_2_acc: 0.7625 - val_loss: 0.7776 - val_classification_1_loss: 0.2574 - val_classification_2_loss: 0.5214 - val_classification_1_acc: 0.8950 - val_classification_2_acc: 0.7425\n",
      "Epoch 86/150\n",
      "400/400 [==============================] - 0s 187us/sample - loss: 0.7114 - classification_1_loss: 0.2079 - classification_2_loss: 0.5103 - classification_1_acc: 0.9200 - classification_2_acc: 0.7600 - val_loss: 0.7759 - val_classification_1_loss: 0.2548 - val_classification_2_loss: 0.5231 - val_classification_1_acc: 0.8975 - val_classification_2_acc: 0.7400\n",
      "Epoch 87/150\n",
      "400/400 [==============================] - 0s 250us/sample - loss: 0.7096 - classification_1_loss: 0.2032 - classification_2_loss: 0.5117 - classification_1_acc: 0.9200 - classification_2_acc: 0.7625 - val_loss: 0.7746 - val_classification_1_loss: 0.2529 - val_classification_2_loss: 0.5209 - val_classification_1_acc: 0.8975 - val_classification_2_acc: 0.7450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/150\n",
      "400/400 [==============================] - 0s 241us/sample - loss: 0.7070 - classification_1_loss: 0.1957 - classification_2_loss: 0.5058 - classification_1_acc: 0.9200 - classification_2_acc: 0.7625 - val_loss: 0.7730 - val_classification_1_loss: 0.2519 - val_classification_2_loss: 0.5235 - val_classification_1_acc: 0.8950 - val_classification_2_acc: 0.7375\n",
      "Epoch 89/150\n",
      "400/400 [==============================] - 0s 250us/sample - loss: 0.7051 - classification_1_loss: 0.1980 - classification_2_loss: 0.5055 - classification_1_acc: 0.9200 - classification_2_acc: 0.7650 - val_loss: 0.7710 - val_classification_1_loss: 0.2542 - val_classification_2_loss: 0.5269 - val_classification_1_acc: 0.8950 - val_classification_2_acc: 0.7375\n",
      "Epoch 90/150\n",
      "400/400 [==============================] - 0s 144us/sample - loss: 0.7031 - classification_1_loss: 0.1942 - classification_2_loss: 0.5095 - classification_1_acc: 0.9225 - classification_2_acc: 0.7600 - val_loss: 0.7693 - val_classification_1_loss: 0.2480 - val_classification_2_loss: 0.5148 - val_classification_1_acc: 0.9000 - val_classification_2_acc: 0.7400\n",
      "Epoch 91/150\n",
      "400/400 [==============================] - 0s 158us/sample - loss: 0.7011 - classification_1_loss: 0.2019 - classification_2_loss: 0.5073 - classification_1_acc: 0.9200 - classification_2_acc: 0.7625 - val_loss: 0.7678 - val_classification_1_loss: 0.2551 - val_classification_2_loss: 0.5148 - val_classification_1_acc: 0.8975 - val_classification_2_acc: 0.7350\n",
      "Epoch 92/150\n",
      "400/400 [==============================] - 0s 130us/sample - loss: 0.6983 - classification_1_loss: 0.1940 - classification_2_loss: 0.5022 - classification_1_acc: 0.9250 - classification_2_acc: 0.7650 - val_loss: 0.7668 - val_classification_1_loss: 0.2477 - val_classification_2_loss: 0.5177 - val_classification_1_acc: 0.9000 - val_classification_2_acc: 0.7400\n",
      "Epoch 93/150\n",
      "400/400 [==============================] - 0s 274us/sample - loss: 0.6962 - classification_1_loss: 0.1893 - classification_2_loss: 0.5023 - classification_1_acc: 0.9225 - classification_2_acc: 0.7650 - val_loss: 0.7647 - val_classification_1_loss: 0.2478 - val_classification_2_loss: 0.5222 - val_classification_1_acc: 0.9000 - val_classification_2_acc: 0.7350\n",
      "Epoch 94/150\n",
      "400/400 [==============================] - 0s 286us/sample - loss: 0.6940 - classification_1_loss: 0.1883 - classification_2_loss: 0.5007 - classification_1_acc: 0.9225 - classification_2_acc: 0.7650 - val_loss: 0.7634 - val_classification_1_loss: 0.2523 - val_classification_2_loss: 0.5147 - val_classification_1_acc: 0.9000 - val_classification_2_acc: 0.7400\n",
      "Epoch 95/150\n",
      "400/400 [==============================] - 0s 106us/sample - loss: 0.6924 - classification_1_loss: 0.1918 - classification_2_loss: 0.5058 - classification_1_acc: 0.9225 - classification_2_acc: 0.7650 - val_loss: 0.7621 - val_classification_1_loss: 0.2435 - val_classification_2_loss: 0.5129 - val_classification_1_acc: 0.9000 - val_classification_2_acc: 0.7425\n",
      "Epoch 96/150\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 0.6904 - classification_1_loss: 0.1881 - classification_2_loss: 0.4948 - classification_1_acc: 0.9250 - classification_2_acc: 0.7675 - val_loss: 0.7598 - val_classification_1_loss: 0.2428 - val_classification_2_loss: 0.5101 - val_classification_1_acc: 0.8950 - val_classification_2_acc: 0.7450\n",
      "Epoch 97/150\n",
      "400/400 [==============================] - 0s 126us/sample - loss: 0.6887 - classification_1_loss: 0.1895 - classification_2_loss: 0.4980 - classification_1_acc: 0.9250 - classification_2_acc: 0.7700 - val_loss: 0.7588 - val_classification_1_loss: 0.2477 - val_classification_2_loss: 0.5110 - val_classification_1_acc: 0.8950 - val_classification_2_acc: 0.7425\n",
      "Epoch 98/150\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.6865 - classification_1_loss: 0.1951 - classification_2_loss: 0.4951 - classification_1_acc: 0.9250 - classification_2_acc: 0.7675 - val_loss: 0.7570 - val_classification_1_loss: 0.2409 - val_classification_2_loss: 0.5125 - val_classification_1_acc: 0.9000 - val_classification_2_acc: 0.7450\n",
      "Epoch 99/150\n",
      "400/400 [==============================] - 0s 123us/sample - loss: 0.6856 - classification_1_loss: 0.1858 - classification_2_loss: 0.5009 - classification_1_acc: 0.9250 - classification_2_acc: 0.7675 - val_loss: 0.7564 - val_classification_1_loss: 0.2474 - val_classification_2_loss: 0.5140 - val_classification_1_acc: 0.9025 - val_classification_2_acc: 0.7450\n",
      "Epoch 100/150\n",
      "400/400 [==============================] - 0s 116us/sample - loss: 0.6829 - classification_1_loss: 0.1854 - classification_2_loss: 0.5020 - classification_1_acc: 0.9250 - classification_2_acc: 0.7625 - val_loss: 0.7538 - val_classification_1_loss: 0.2433 - val_classification_2_loss: 0.5097 - val_classification_1_acc: 0.9000 - val_classification_2_acc: 0.7500\n",
      "Epoch 101/150\n",
      "400/400 [==============================] - 0s 111us/sample - loss: 0.6811 - classification_1_loss: 0.1958 - classification_2_loss: 0.4911 - classification_1_acc: 0.9250 - classification_2_acc: 0.7675 - val_loss: 0.7521 - val_classification_1_loss: 0.2434 - val_classification_2_loss: 0.5101 - val_classification_1_acc: 0.9000 - val_classification_2_acc: 0.7525\n",
      "Epoch 102/150\n",
      "400/400 [==============================] - 0s 94us/sample - loss: 0.6801 - classification_1_loss: 0.1885 - classification_2_loss: 0.4916 - classification_1_acc: 0.9225 - classification_2_acc: 0.7650 - val_loss: 0.7514 - val_classification_1_loss: 0.2460 - val_classification_2_loss: 0.5091 - val_classification_1_acc: 0.9050 - val_classification_2_acc: 0.7550\n",
      "Epoch 103/150\n",
      "400/400 [==============================] - 0s 123us/sample - loss: 0.6777 - classification_1_loss: 0.1856 - classification_2_loss: 0.4931 - classification_1_acc: 0.9225 - classification_2_acc: 0.7675 - val_loss: 0.7493 - val_classification_1_loss: 0.2480 - val_classification_2_loss: 0.5071 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7550\n",
      "Epoch 104/150\n",
      "400/400 [==============================] - 0s 134us/sample - loss: 0.6758 - classification_1_loss: 0.1886 - classification_2_loss: 0.4991 - classification_1_acc: 0.9225 - classification_2_acc: 0.7650 - val_loss: 0.7485 - val_classification_1_loss: 0.2397 - val_classification_2_loss: 0.5019 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7550\n",
      "Epoch 105/150\n",
      "400/400 [==============================] - 0s 138us/sample - loss: 0.6751 - classification_1_loss: 0.1832 - classification_2_loss: 0.4884 - classification_1_acc: 0.9225 - classification_2_acc: 0.7725 - val_loss: 0.7477 - val_classification_1_loss: 0.2410 - val_classification_2_loss: 0.5000 - val_classification_1_acc: 0.9050 - val_classification_2_acc: 0.7575\n",
      "Epoch 106/150\n",
      "400/400 [==============================] - 0s 148us/sample - loss: 0.6732 - classification_1_loss: 0.1813 - classification_2_loss: 0.4897 - classification_1_acc: 0.9275 - classification_2_acc: 0.7700 - val_loss: 0.7464 - val_classification_1_loss: 0.2448 - val_classification_2_loss: 0.5018 - val_classification_1_acc: 0.9050 - val_classification_2_acc: 0.7550\n",
      "Epoch 107/150\n",
      "400/400 [==============================] - 0s 100us/sample - loss: 0.6709 - classification_1_loss: 0.1772 - classification_2_loss: 0.4906 - classification_1_acc: 0.9250 - classification_2_acc: 0.7725 - val_loss: 0.7444 - val_classification_1_loss: 0.2428 - val_classification_2_loss: 0.4993 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7600\n",
      "Epoch 108/150\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.6694 - classification_1_loss: 0.1797 - classification_2_loss: 0.4932 - classification_1_acc: 0.9275 - classification_2_acc: 0.7725 - val_loss: 0.7420 - val_classification_1_loss: 0.2342 - val_classification_2_loss: 0.5034 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7600\n",
      "Epoch 109/150\n",
      "400/400 [==============================] - 0s 123us/sample - loss: 0.6678 - classification_1_loss: 0.1860 - classification_2_loss: 0.4925 - classification_1_acc: 0.9250 - classification_2_acc: 0.7700 - val_loss: 0.7424 - val_classification_1_loss: 0.2407 - val_classification_2_loss: 0.5001 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/150\n",
      "400/400 [==============================] - 0s 131us/sample - loss: 0.6664 - classification_1_loss: 0.1762 - classification_2_loss: 0.4901 - classification_1_acc: 0.9250 - classification_2_acc: 0.7700 - val_loss: 0.7421 - val_classification_1_loss: 0.2408 - val_classification_2_loss: 0.4942 - val_classification_1_acc: 0.9050 - val_classification_2_acc: 0.7600\n",
      "Epoch 111/150\n",
      "400/400 [==============================] - 0s 78us/sample - loss: 0.6643 - classification_1_loss: 0.1781 - classification_2_loss: 0.4857 - classification_1_acc: 0.9275 - classification_2_acc: 0.7725 - val_loss: 0.7392 - val_classification_1_loss: 0.2397 - val_classification_2_loss: 0.5035 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7625\n",
      "Epoch 112/150\n",
      "400/400 [==============================] - 0s 95us/sample - loss: 0.6629 - classification_1_loss: 0.1743 - classification_2_loss: 0.4843 - classification_1_acc: 0.9250 - classification_2_acc: 0.7775 - val_loss: 0.7372 - val_classification_1_loss: 0.2404 - val_classification_2_loss: 0.5051 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7625\n",
      "Epoch 113/150\n",
      "400/400 [==============================] - 0s 96us/sample - loss: 0.6619 - classification_1_loss: 0.1780 - classification_2_loss: 0.4832 - classification_1_acc: 0.9250 - classification_2_acc: 0.7750 - val_loss: 0.7357 - val_classification_1_loss: 0.2354 - val_classification_2_loss: 0.4935 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7675\n",
      "Epoch 114/150\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 0.6610 - classification_1_loss: 0.1710 - classification_2_loss: 0.4863 - classification_1_acc: 0.9250 - classification_2_acc: 0.7750 - val_loss: 0.7352 - val_classification_1_loss: 0.2401 - val_classification_2_loss: 0.4976 - val_classification_1_acc: 0.9075 - val_classification_2_acc: 0.7675\n",
      "Epoch 115/150\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 0.6590 - classification_1_loss: 0.1725 - classification_2_loss: 0.4952 - classification_1_acc: 0.9250 - classification_2_acc: 0.7675 - val_loss: 0.7351 - val_classification_1_loss: 0.2445 - val_classification_2_loss: 0.5007 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7600\n",
      "Epoch 116/150\n",
      "400/400 [==============================] - 0s 99us/sample - loss: 0.6580 - classification_1_loss: 0.1742 - classification_2_loss: 0.4843 - classification_1_acc: 0.9275 - classification_2_acc: 0.7725 - val_loss: 0.7347 - val_classification_1_loss: 0.2355 - val_classification_2_loss: 0.4971 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7575\n",
      "Epoch 117/150\n",
      "400/400 [==============================] - 0s 100us/sample - loss: 0.6564 - classification_1_loss: 0.1767 - classification_2_loss: 0.4823 - classification_1_acc: 0.9275 - classification_2_acc: 0.7725 - val_loss: 0.7318 - val_classification_1_loss: 0.2441 - val_classification_2_loss: 0.4903 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7625\n",
      "Epoch 118/150\n",
      "400/400 [==============================] - 0s 85us/sample - loss: 0.6553 - classification_1_loss: 0.1714 - classification_2_loss: 0.4786 - classification_1_acc: 0.9275 - classification_2_acc: 0.7725 - val_loss: 0.7304 - val_classification_1_loss: 0.2383 - val_classification_2_loss: 0.4958 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7700\n",
      "Epoch 119/150\n",
      "400/400 [==============================] - 0s 98us/sample - loss: 0.6541 - classification_1_loss: 0.1696 - classification_2_loss: 0.4911 - classification_1_acc: 0.9250 - classification_2_acc: 0.7750 - val_loss: 0.7309 - val_classification_1_loss: 0.2303 - val_classification_2_loss: 0.4977 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7625\n",
      "Epoch 120/150\n",
      "400/400 [==============================] - 0s 71us/sample - loss: 0.6521 - classification_1_loss: 0.1758 - classification_2_loss: 0.4818 - classification_1_acc: 0.9275 - classification_2_acc: 0.7750 - val_loss: 0.7281 - val_classification_1_loss: 0.2373 - val_classification_2_loss: 0.4913 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7725\n",
      "Epoch 121/150\n",
      "400/400 [==============================] - 0s 100us/sample - loss: 0.6517 - classification_1_loss: 0.1680 - classification_2_loss: 0.4844 - classification_1_acc: 0.9275 - classification_2_acc: 0.7725 - val_loss: 0.7295 - val_classification_1_loss: 0.2315 - val_classification_2_loss: 0.4926 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7675\n",
      "Epoch 122/150\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.6676 - classification_1_loss: 0.1942 - classification_2_loss: 0.4735 - classification_1_acc: 0.9062 - classification_2_acc: 0.718 - 0s 73us/sample - loss: 0.6500 - classification_1_loss: 0.1651 - classification_2_loss: 0.4828 - classification_1_acc: 0.9275 - classification_2_acc: 0.7725 - val_loss: 0.7278 - val_classification_1_loss: 0.2410 - val_classification_2_loss: 0.4922 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7675\n",
      "Epoch 123/150\n",
      "400/400 [==============================] - 0s 113us/sample - loss: 0.6497 - classification_1_loss: 0.1710 - classification_2_loss: 0.4783 - classification_1_acc: 0.9275 - classification_2_acc: 0.7775 - val_loss: 0.7248 - val_classification_1_loss: 0.2290 - val_classification_2_loss: 0.5014 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7725\n",
      "Epoch 124/150\n",
      "400/400 [==============================] - 0s 112us/sample - loss: 0.6487 - classification_1_loss: 0.1700 - classification_2_loss: 0.4817 - classification_1_acc: 0.9275 - classification_2_acc: 0.7750 - val_loss: 0.7263 - val_classification_1_loss: 0.2371 - val_classification_2_loss: 0.4897 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7675\n",
      "Epoch 125/150\n",
      "400/400 [==============================] - 0s 108us/sample - loss: 0.6461 - classification_1_loss: 0.1669 - classification_2_loss: 0.4747 - classification_1_acc: 0.9275 - classification_2_acc: 0.7750 - val_loss: 0.7243 - val_classification_1_loss: 0.2275 - val_classification_2_loss: 0.4955 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7700\n",
      "Epoch 126/150\n",
      "400/400 [==============================] - 0s 115us/sample - loss: 0.6451 - classification_1_loss: 0.1708 - classification_2_loss: 0.4782 - classification_1_acc: 0.9275 - classification_2_acc: 0.7775 - val_loss: 0.7230 - val_classification_1_loss: 0.2298 - val_classification_2_loss: 0.4871 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7725\n",
      "Epoch 127/150\n",
      "400/400 [==============================] - 0s 137us/sample - loss: 0.6441 - classification_1_loss: 0.1625 - classification_2_loss: 0.4760 - classification_1_acc: 0.9275 - classification_2_acc: 0.7750 - val_loss: 0.7224 - val_classification_1_loss: 0.2346 - val_classification_2_loss: 0.4827 - val_classification_1_acc: 0.9150 - val_classification_2_acc: 0.7700\n",
      "Epoch 128/150\n",
      "400/400 [==============================] - 0s 215us/sample - loss: 0.6437 - classification_1_loss: 0.1653 - classification_2_loss: 0.4700 - classification_1_acc: 0.9275 - classification_2_acc: 0.7725 - val_loss: 0.7213 - val_classification_1_loss: 0.2283 - val_classification_2_loss: 0.4869 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7700\n",
      "Epoch 129/150\n",
      "400/400 [==============================] - 0s 204us/sample - loss: 0.6418 - classification_1_loss: 0.1686 - classification_2_loss: 0.4794 - classification_1_acc: 0.9275 - classification_2_acc: 0.7725 - val_loss: 0.7233 - val_classification_1_loss: 0.2347 - val_classification_2_loss: 0.4949 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7700\n",
      "Epoch 130/150\n",
      "400/400 [==============================] - 0s 176us/sample - loss: 0.6411 - classification_1_loss: 0.1685 - classification_2_loss: 0.4815 - classification_1_acc: 0.9300 - classification_2_acc: 0.7800 - val_loss: 0.7202 - val_classification_1_loss: 0.2280 - val_classification_2_loss: 0.4845 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7725\n",
      "Epoch 131/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 0s 175us/sample - loss: 0.6395 - classification_1_loss: 0.1697 - classification_2_loss: 0.4718 - classification_1_acc: 0.9325 - classification_2_acc: 0.7800 - val_loss: 0.7199 - val_classification_1_loss: 0.2301 - val_classification_2_loss: 0.4903 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7675\n",
      "Epoch 132/150\n",
      "400/400 [==============================] - 0s 211us/sample - loss: 0.6390 - classification_1_loss: 0.1650 - classification_2_loss: 0.4677 - classification_1_acc: 0.9300 - classification_2_acc: 0.7800 - val_loss: 0.7202 - val_classification_1_loss: 0.2322 - val_classification_2_loss: 0.4851 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7675\n",
      "Epoch 133/150\n",
      "400/400 [==============================] - 0s 127us/sample - loss: 0.6381 - classification_1_loss: 0.1631 - classification_2_loss: 0.4692 - classification_1_acc: 0.9300 - classification_2_acc: 0.7750 - val_loss: 0.7188 - val_classification_1_loss: 0.2309 - val_classification_2_loss: 0.4884 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7675\n",
      "Epoch 134/150\n",
      "400/400 [==============================] - 0s 162us/sample - loss: 0.6365 - classification_1_loss: 0.1675 - classification_2_loss: 0.4665 - classification_1_acc: 0.9325 - classification_2_acc: 0.7750 - val_loss: 0.7163 - val_classification_1_loss: 0.2299 - val_classification_2_loss: 0.4878 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7675\n",
      "Epoch 135/150\n",
      "400/400 [==============================] - 0s 128us/sample - loss: 0.6366 - classification_1_loss: 0.1649 - classification_2_loss: 0.4683 - classification_1_acc: 0.9275 - classification_2_acc: 0.7750 - val_loss: 0.7159 - val_classification_1_loss: 0.2367 - val_classification_2_loss: 0.4784 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7700\n",
      "Epoch 136/150\n",
      "400/400 [==============================] - 0s 198us/sample - loss: 0.6355 - classification_1_loss: 0.1618 - classification_2_loss: 0.4713 - classification_1_acc: 0.9275 - classification_2_acc: 0.7750 - val_loss: 0.7167 - val_classification_1_loss: 0.2261 - val_classification_2_loss: 0.4909 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7700\n",
      "Epoch 137/150\n",
      "400/400 [==============================] - 0s 129us/sample - loss: 0.6339 - classification_1_loss: 0.1661 - classification_2_loss: 0.4713 - classification_1_acc: 0.9325 - classification_2_acc: 0.7725 - val_loss: 0.7132 - val_classification_1_loss: 0.2259 - val_classification_2_loss: 0.4845 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7700\n",
      "Epoch 138/150\n",
      "400/400 [==============================] - 0s 160us/sample - loss: 0.6337 - classification_1_loss: 0.1607 - classification_2_loss: 0.4734 - classification_1_acc: 0.9325 - classification_2_acc: 0.7725 - val_loss: 0.7130 - val_classification_1_loss: 0.2315 - val_classification_2_loss: 0.4909 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7725\n",
      "Epoch 139/150\n",
      "400/400 [==============================] - 0s 146us/sample - loss: 0.6322 - classification_1_loss: 0.1606 - classification_2_loss: 0.4688 - classification_1_acc: 0.9325 - classification_2_acc: 0.7725 - val_loss: 0.7144 - val_classification_1_loss: 0.2305 - val_classification_2_loss: 0.4875 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7725\n",
      "Epoch 140/150\n",
      "400/400 [==============================] - 0s 237us/sample - loss: 0.6312 - classification_1_loss: 0.1657 - classification_2_loss: 0.4699 - classification_1_acc: 0.9325 - classification_2_acc: 0.7775 - val_loss: 0.7130 - val_classification_1_loss: 0.2423 - val_classification_2_loss: 0.4848 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7725\n",
      "Epoch 141/150\n",
      "400/400 [==============================] - 0s 314us/sample - loss: 0.6302 - classification_1_loss: 0.1565 - classification_2_loss: 0.4623 - classification_1_acc: 0.9300 - classification_2_acc: 0.7725 - val_loss: 0.7118 - val_classification_1_loss: 0.2311 - val_classification_2_loss: 0.4873 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7750\n",
      "Epoch 142/150\n",
      "400/400 [==============================] - 0s 163us/sample - loss: 0.6317 - classification_1_loss: 0.1630 - classification_2_loss: 0.4754 - classification_1_acc: 0.9300 - classification_2_acc: 0.7725 - val_loss: 0.7130 - val_classification_1_loss: 0.2307 - val_classification_2_loss: 0.4845 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7650\n",
      "Epoch 143/150\n",
      "400/400 [==============================] - 0s 148us/sample - loss: 0.6286 - classification_1_loss: 0.1599 - classification_2_loss: 0.4785 - classification_1_acc: 0.9300 - classification_2_acc: 0.7750 - val_loss: 0.7096 - val_classification_1_loss: 0.2219 - val_classification_2_loss: 0.4810 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7725\n",
      "Epoch 144/150\n",
      "400/400 [==============================] - 0s 249us/sample - loss: 0.6275 - classification_1_loss: 0.1631 - classification_2_loss: 0.4700 - classification_1_acc: 0.9300 - classification_2_acc: 0.7775 - val_loss: 0.7092 - val_classification_1_loss: 0.2327 - val_classification_2_loss: 0.4855 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7775\n",
      "Epoch 145/150\n",
      "400/400 [==============================] - 0s 174us/sample - loss: 0.6268 - classification_1_loss: 0.1649 - classification_2_loss: 0.4617 - classification_1_acc: 0.9300 - classification_2_acc: 0.7750 - val_loss: 0.7100 - val_classification_1_loss: 0.2274 - val_classification_2_loss: 0.4781 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7775\n",
      "Epoch 146/150\n",
      "400/400 [==============================] - 0s 131us/sample - loss: 0.6258 - classification_1_loss: 0.1601 - classification_2_loss: 0.4700 - classification_1_acc: 0.9350 - classification_2_acc: 0.7775 - val_loss: 0.7079 - val_classification_1_loss: 0.2246 - val_classification_2_loss: 0.4792 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7775\n",
      "Epoch 147/150\n",
      "400/400 [==============================] - 0s 98us/sample - loss: 0.6250 - classification_1_loss: 0.1603 - classification_2_loss: 0.4674 - classification_1_acc: 0.9325 - classification_2_acc: 0.7775 - val_loss: 0.7085 - val_classification_1_loss: 0.2255 - val_classification_2_loss: 0.4815 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7750\n",
      "Epoch 148/150\n",
      "400/400 [==============================] - 0s 246us/sample - loss: 0.6244 - classification_1_loss: 0.1559 - classification_2_loss: 0.4688 - classification_1_acc: 0.9325 - classification_2_acc: 0.7775 - val_loss: 0.7073 - val_classification_1_loss: 0.2279 - val_classification_2_loss: 0.4791 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7800\n",
      "Epoch 149/150\n",
      "400/400 [==============================] - 0s 259us/sample - loss: 0.6248 - classification_1_loss: 0.1573 - classification_2_loss: 0.4632 - classification_1_acc: 0.9300 - classification_2_acc: 0.7775 - val_loss: 0.7084 - val_classification_1_loss: 0.2245 - val_classification_2_loss: 0.4864 - val_classification_1_acc: 0.9100 - val_classification_2_acc: 0.7700\n",
      "Epoch 150/150\n",
      "400/400 [==============================] - 0s 176us/sample - loss: 0.6229 - classification_1_loss: 0.1596 - classification_2_loss: 0.4635 - classification_1_acc: 0.9350 - classification_2_acc: 0.7775 - val_loss: 0.7052 - val_classification_1_loss: 0.2198 - val_classification_2_loss: 0.4826 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2c978ba8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=150,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "* Does it surprise you that the loss for label 1 is smaller than the loss for label 2?    \n",
    "* Quick side check... do we overtrain?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Custom Layers\n",
    "\n",
    "Let's define a simple custom layer which is nothing but an affine transformation followed by a relu activation. We need to define:\n",
    "* \\_\\_init\\_\\_()      \n",
    "* build()   \n",
    "* call()   \n",
    "* compute_output_shape()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class MyStupidDenseLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):   \n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super(MyStupidDenseLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_variable(\"W\",\n",
    "                                    shape=[int(input_shape[1]),self.output_dim], \n",
    "                                   initializer=tf.glorot_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "        self.b = self.add_variable(\"b\",\n",
    "                                    shape=[self.output_dim, ], \n",
    "                                   initializer=tf.initializers.zeros(),\n",
    "                                   trainable=True)\n",
    "        super(MyStupidDenseLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.nn.relu(tf.tensordot(x, self.W, axes=[[1],[0]]) + self.b)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our custom layer in the same architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input layer(s)\n",
    "input_numbers = tf.keras.layers.Input(shape=(10,), name=\"input_numbers\")\n",
    "\n",
    "# Define separate hidden layers and acting on (same!) input\n",
    "dense_layer_1 = MyStupidDenseLayer(10, name='dense_1')\n",
    "dense_layer_2 = MyStupidDenseLayer(10, name='dense_2')\n",
    "\n",
    "dense_output_1 = dense_layer_1(input_numbers)\n",
    "dense_output_2 = dense_layer_2(input_numbers)\n",
    "\n",
    "\n",
    "\n",
    "# Define classification layer and act on previous output to classify examples\n",
    "classification_layer_1 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_1')\n",
    "\n",
    "classification_layer_2 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_2')\n",
    "\n",
    "\n",
    "classification_output_1 = classification_layer_1(dense_output_1)\n",
    "classification_output_2 = classification_layer_2(dense_output_2)\n",
    "\n",
    "\n",
    "\n",
    "# Build and compile model\n",
    "\n",
    "model_input = input_numbers\n",
    "model_output = [classification_output_1, classification_output_2]\n",
    "losses = ['binary_crossentropy', 'binary_crossentropy']\n",
    "metrics = ['acc', 'acc']\n",
    "\n",
    "model = tf.keras.models.Model(model_input, model_output)\n",
    "model.compile(loss=losses,  optimizer='adam', metrics=metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 100us/sample - loss: 0.6128 - classification_1_loss: 0.1580 - classification_2_loss: 0.4571 - classification_1_acc: 0.9325 - classification_2_acc: 0.7800 - val_loss: 0.6883 - val_classification_1_loss: 0.2224 - val_classification_2_loss: 0.4668 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2d8e0940>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=149,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close enough. So the custom layer works exactly as expected.\n",
    "\n",
    "\n",
    "### 6. Sharing & Freezing Weights \n",
    "\n",
    "\n",
    "Let's define an identical model. The weights for this model will be re-initialized, so upon training for 1 epoch the loss will be much higher than after 150 epochs for the original model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input layer(s)\n",
    "input_numbers = tf.keras.layers.Input(shape=(10,), name=\"input_numbers\")\n",
    "\n",
    "# Define separate hidden layers and acting on (same!) input\n",
    "dense_layer_1 = MyStupidDenseLayer(10, name='dense_1')\n",
    "dense_layer_2 = MyStupidDenseLayer(10, name='dense_2')\n",
    "\n",
    "dense_output_1 = dense_layer_1(input_numbers)\n",
    "dense_output_2 = dense_layer_2(input_numbers)\n",
    "\n",
    "\n",
    "\n",
    "# Define classification layer and act on previous output to classify examples\n",
    "classification_layer_1 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_1')\n",
    "\n",
    "classification_layer_2 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_2')\n",
    "\n",
    "\n",
    "classification_output_1 = classification_layer_1(dense_output_1)\n",
    "classification_output_2 = classification_layer_2(dense_output_2)\n",
    "\n",
    "\n",
    "\n",
    "# Build and compile model\n",
    "\n",
    "model_input = input_numbers\n",
    "model_output = [classification_output_1, classification_output_2]\n",
    "losses = ['binary_crossentropy', 'binary_crossentropy']\n",
    "metrics = ['acc', 'acc']\n",
    "\n",
    "model_mirror = tf.keras.models.Model(model_input, model_output)\n",
    "model_mirror.compile(loss=losses,  optimizer='adam', metrics=metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 1s 1ms/sample - loss: 1.4418 - classification_1_loss: 0.7707 - classification_2_loss: 0.6700 - classification_1_acc: 0.3250 - classification_2_acc: 0.5900 - val_loss: 1.4239 - val_classification_1_loss: 0.7571 - val_classification_2_loss: 0.6641 - val_classification_1_acc: 0.3525 - val_classification_2_acc: 0.6300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2df1a6a0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, this model would make very different predictions - the loss is ~1.4 vs ~0.6. No surprise, obviously, as this model has not been trained.\n",
    "\n",
    "What if we manually set the the weights of the second model to the weights of the first? Let's look at the layers for both models and then copy the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0xb2d77e518>,\n",
       " <__main__.MyStupidDenseLayer at 0xb2d91a588>,\n",
       " <__main__.MyStupidDenseLayer at 0xb2d91a748>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x109bf3780>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x109bf3908>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one layer in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would that be? No magic.. weights and biases of layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(model.layers[1].get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(model.layers[1].get_weights()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. What is the name of the layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_1'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct. Is it trainable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Are the layers of the second model essentially the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0xb2df57908>,\n",
       " <__main__.MyStupidDenseLayer at 0xb2df57978>,\n",
       " <__main__.MyStupidDenseLayer at 0xb2df57b70>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0xb2df57fd0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0xb2df1aa58>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup. (We could look at shapes, etc, but won't do that now.) Now we set the second model's weights to equal the first model's trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mirror.layers[1].set_weights(model.layers[1].get_weights())\n",
    "model_mirror.layers[2].set_weights(model.layers[2].get_weights())\n",
    "model_mirror.layers[3].set_weights(model.layers[3].get_weights())\n",
    "model_mirror.layers[4].set_weights(model.layers[4].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why don't we set the weights for layer 0?\n",
    "\n",
    "Okay. What is the loss now for the mirror model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 248us/sample - loss: 0.6146 - classification_1_loss: 0.1575 - classification_2_loss: 0.4642 - classification_1_acc: 0.9300 - classification_2_acc: 0.7825 - val_loss: 0.6874 - val_classification_1_loss: 0.2141 - val_classification_2_loss: 0.4658 - val_classification_1_acc: 0.9125 - val_classification_2_acc: 0.7825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2e6c26a0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Essentially the same as for the original one! (Not a surprise actually... but great that it works.)\n",
    "\n",
    "Let's now turn to **freezing** layers. For example, you may want to copy weights from another model but hold those weights fixed upon further training. To test this, we will do a simple toy exercise: freeze a layer of the second model and train it further. Let's compare some weights before and after training. \n",
    "\n",
    "We start by defining a new model that again has the same architecture, **but we set the trainable-parameter for one layer to False**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input layer(s)\n",
    "input_numbers = tf.keras.layers.Input(shape=(10,), name=\"input_numbers\")\n",
    "\n",
    "# Define separate hidden layers and acting on (same!) input\n",
    "dense_layer_1 = MyStupidDenseLayer(10, name='dense_1')\n",
    "\n",
    "dense_layer_2 = MyStupidDenseLayer(10, name='dense_2')\n",
    "dense_layer_2.trainable = False                                   # Freeze this layer\n",
    "\n",
    "dense_output_1 = dense_layer_1(input_numbers)\n",
    "dense_output_2 = dense_layer_2(input_numbers)\n",
    "\n",
    "\n",
    "\n",
    "# Define classification layer and act on previous output to classify examples\n",
    "classification_layer_1 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_1')\n",
    "\n",
    "classification_layer_2 = tf.keras.layers.Dense(1, \n",
    "                            activation='sigmoid', name='classification_2')\n",
    "\n",
    "\n",
    "\n",
    "classification_output_1 = classification_layer_1(dense_output_1)\n",
    "classification_output_2 = classification_layer_2(dense_output_2)\n",
    "\n",
    "\n",
    "\n",
    "# Build and compile model\n",
    "\n",
    "model_input = input_numbers\n",
    "model_output = [classification_output_1, classification_output_2]\n",
    "losses = ['binary_crossentropy', 'binary_crossentropy']\n",
    "metrics = ['acc', 'acc']\n",
    "\n",
    "model_mirror_2 = tf.keras.models.Model(model_input, model_output)\n",
    "model_mirror_2.compile(loss=losses,  optimizer='adam', metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we again set the weights to mirror the first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mirror_2.layers[1].set_weights(model.layers[1].get_weights())\n",
    "model_mirror_2.layers[2].set_weights(model.layers[2].get_weights())\n",
    "model_mirror_2.layers[3].set_weights(model.layers[3].get_weights())\n",
    "model_mirror_2.layers[4].set_weights(model.layers[4].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are (some of) the weights now, before we train further?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0916228 ,  0.0080574 , -0.6700347 ],\n",
       "       [ 0.36431167, -0.7220702 , -0.32361656],\n",
       "       [ 0.32802176,  0.30155778, -0.45293397]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[1].get_weights()[0][:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.59885055, -0.04448628, -0.04327861],\n",
       "       [-0.55413663,  0.1483072 ,  0.22182831],\n",
       "       [-0.03545466,  0.5287597 , -0.05535174]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[2].get_weights()[0][:3, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare that with the values post further training. Let's also check the 'trainable' settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[1].trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[2].trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, so it shows that one hidden layer supposedly is trainable, the other one is not. Is that true? We train more and compare weights compared to what they were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 400 samples\n",
      "400/400 [==============================] - 0s 84us/sample - loss: 0.5971 - classification_1_loss: 0.1468 - classification_2_loss: 0.4534 - classification_1_acc: 0.9300 - classification_2_acc: 0.7800 - val_loss: 0.6748 - val_classification_1_loss: 0.2097 - val_classification_2_loss: 0.4654 - val_classification_1_acc: 0.9250 - val_classification_2_acc: 0.7900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2e6c2b00>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=40,\n",
    "    verbose=0\n",
    ")\n",
    "model_mirror_2.fit(\n",
    "    train_X,\n",
    "    [train_a_y, train_b_y],\n",
    "    validation_data=[test_X, [test_a_y, test_b_y]],\n",
    "    epochs=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1080302 , -0.01284085, -0.70266557],\n",
       "       [ 0.35729578, -0.7506223 , -0.33273327],\n",
       "       [ 0.36315778,  0.3460176 , -0.4623005 ]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[1].get_weights()[0][:3, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights **did change**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.59885055, -0.04448628, -0.04327861],\n",
       "       [-0.55413663,  0.1483072 ,  0.22182831],\n",
       "       [-0.03545466,  0.5287597 , -0.05535174]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mirror_2.layers[2].get_weights()[0][:3, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights **did not change**. So here we could see layer freezing at work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1_14] *",
   "language": "python",
   "name": "conda-env-tf1_14-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
